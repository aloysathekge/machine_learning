{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea62e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8a8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 1: UNDERSTANDING HYPERPARAMETERS\n",
      "================================================================================\n",
      "\n",
      "WHAT ARE HYPERPARAMETERS?\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Parameters (learned automatically):\n",
      "    â†’ Weights and biases in your network\n",
      "    â†’ Updated via backpropagation\n",
      "    â†’ Example: W in y = Wx + b\n",
      "\n",
      "Hyperparameters (YOU must set):\n",
      "    â†’ Control the learning process\n",
      "    â†’ Not learned from data\n",
      "    â†’ Examples: learning rate, batch size, number of layers\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    KEY HYPERPARAMETERS                         â•‘\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘ 1. Learning Rate        â†’ How big are weight updates?         â•‘\n",
      "â•‘ 2. Batch Size           â†’ Samples per gradient update         â•‘\n",
      "â•‘ 3. Number of Epochs     â†’ Training iterations over data       â•‘\n",
      "â•‘ 4. Optimizer            â†’ SGD, Adam, AdamW, etc.              â•‘\n",
      "â•‘ 5. Network Architecture â†’ Layers, neurons, activations        â•‘\n",
      "â•‘ 6. Regularization       â†’ Dropout, weight decay, batch norm   â•‘\n",
      "â•‘ 7. LR Schedule          â†’ How to adjust LR over time          â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "IMPORTANCE RANKING (if you can only tune a few):\n",
      "    ğŸ¥‡ Learning Rate        â­â­â­â­â­ (MOST CRITICAL!)\n",
      "    ğŸ¥ˆ Batch Size           â­â­â­â­\n",
      "    ğŸ¥‰ Architecture         â­â­â­â­\n",
      "    4ï¸âƒ£  Optimizer           â­â­â­\n",
      "    5ï¸âƒ£  Regularization      â­â­â­\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SECTION 1: UNDERSTANDING HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "WHAT ARE HYPERPARAMETERS?\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Parameters (learned automatically):\n",
    "    â†’ Weights and biases in your network\n",
    "    â†’ Updated via backpropagation\n",
    "    â†’ Example: W in y = Wx + b\n",
    "\n",
    "Hyperparameters (YOU must set):\n",
    "    â†’ Control the learning process\n",
    "    â†’ Not learned from data\n",
    "    â†’ Examples: learning rate, batch size, number of layers\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    KEY HYPERPARAMETERS                         â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ 1. Learning Rate        â†’ How big are weight updates?         â•‘\n",
    "â•‘ 2. Batch Size           â†’ Samples per gradient update         â•‘\n",
    "â•‘ 3. Number of Epochs     â†’ Training iterations over data       â•‘\n",
    "â•‘ 4. Optimizer            â†’ SGD, Adam, AdamW, etc.              â•‘\n",
    "â•‘ 5. Network Architecture â†’ Layers, neurons, activations        â•‘\n",
    "â•‘ 6. Regularization       â†’ Dropout, weight decay, batch norm   â•‘\n",
    "â•‘ 7. LR Schedule          â†’ How to adjust LR over time          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "IMPORTANCE RANKING (if you can only tune a few):\n",
    "    ğŸ¥‡ Learning Rate        â­â­â­â­â­ (MOST CRITICAL!)\n",
    "    ğŸ¥ˆ Batch Size           â­â­â­â­\n",
    "    ğŸ¥‰ Architecture         â­â­â­â­\n",
    "    4ï¸âƒ£  Optimizer           â­â­â­\n",
    "    5ï¸âƒ£  Regularization      â­â­â­\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ced702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 2: LEARNING RATE - THE MOST IMPORTANT HYPERPARAMETER\n",
      "================================================================================\n",
      "\n",
      "WHY IS LEARNING RATE SO IMPORTANT?\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "\n",
      "Learning rate controls the step size in gradient descent:\n",
      "    new_weight = old_weight - learning_rate Ã— gradient\n",
      "\n",
      "LEARNING RATE TOO HIGH:  â†’ Oscillates, may explode (NaN)\n",
      "LEARNING RATE TOO LOW:   â†’ Very slow, gets stuck\n",
      "LEARNING RATE JUST RIGHT: â†’ Fast, stable, good performance\n",
      "\n",
      "TYPICAL VALUES:\n",
      "    Adam:    0.0001 to 0.01    (default: 0.001)\n",
      "    SGD:     0.001 to 0.1      (default: 0.01)\n",
      "    AdamW:   0.00001 to 0.001  (default: 0.0001)\n",
      "\n",
      "\n",
      "Demonstration: Effect of Different Learning Rates\n",
      "------------------------------------------------------------\n",
      "  LR = 0.0001: Final loss =   1.4181 âŒ\n",
      "  LR = 0.0010: Final loss =   1.3101 âŒ\n",
      "  LR = 0.0100: Final loss =   1.0119 âŒ\n",
      "  LR = 0.1000: Final loss =   1.0030 âŒ\n",
      "  LR = 1.0000: Final loss =      inf âŒ\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SECTION 2: LEARNING RATE - THE MOST IMPORTANT HYPERPARAMETER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "WHY IS LEARNING RATE SO IMPORTANT?\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Learning rate controls the step size in gradient descent:\n",
    "    new_weight = old_weight - learning_rate Ã— gradient\n",
    "\n",
    "LEARNING RATE TOO HIGH:  â†’ Oscillates, may explode (NaN)\n",
    "LEARNING RATE TOO LOW:   â†’ Very slow, gets stuck\n",
    "LEARNING RATE JUST RIGHT: â†’ Fast, stable, good performance\n",
    "\n",
    "TYPICAL VALUES:\n",
    "    Adam:    0.0001 to 0.01    (default: 0.001)\n",
    "    SGD:     0.001 to 0.1      (default: 0.01)\n",
    "    AdamW:   0.00001 to 0.001  (default: 0.0001)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDemonstration: Effect of Different Learning Rates\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def demonstrate_learning_rates():\n",
    "    \"\"\"Show how different LRs affect training\"\"\"\n",
    "    model = nn.Linear(10, 1)\n",
    "    X = torch.randn(100, 10)\n",
    "    y = torch.randn(100, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "    results = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        model_copy = nn.Linear(10, 1)\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=lr)\n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(100):\n",
    "            optimizer.zero_grad()\n",
    "            output = model_copy(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        results[lr] = losses\n",
    "        final_loss = losses[-1]\n",
    "        status = \"âœ…\" if final_loss < 1.0 else \"âŒ\"\n",
    "        print(f\"  LR = {lr:6.4f}: Final loss = {final_loss:8.4f} {status}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = demonstrate_learning_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a3cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
